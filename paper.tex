\documentclass{vldb}

%%% Packages
\usepackage{graphicx}
\usepackage{balance}
\usepackage{tikz}
\usepackage{standalone}
\usepackage{subfigure}
\usepackage{colortbl}

%%% TikZ libraries
\usetikzlibrary{backgrounds, positioning, decorations.pathreplacing, calc, fit}
\usetikzlibrary{shapes}

%%% Macros
\newcommand{\tbr}{\textbf{[TO BE REVISED]}}
\newcommand{\tbd}{\textbf{[TO BE DONE]}}

\newcommand\diag[4]{%
  \multicolumn{1}{p{#2}|}{\hskip-\tabcolsep
  $\vcenter{\begin{tikzpicture}[baseline=0,anchor=south west,inner sep=#1]
  \path[use as bounding box] (0,0) rectangle (#2+2\tabcolsep,\baselineskip);
  \node[minimum width={#2+2\tabcolsep-\pgflinewidth},
        minimum  height=\baselineskip+\extrarowheight-\pgflinewidth] (box) {};
  \draw[line cap=round] (box.north west) -- (box.south east);
  \node[anchor=south west] at (box.south west) {#3};
  \node[anchor=north east] at (box.north east) {#4};
 \end{tikzpicture}}$\hskip-\tabcolsep}}

\begin{document}

% Title
\title{A Survey on Ordered Binary Decision Diagrams}

% Author(s)
\numberofauthors{1}

\author{
\alignauthor
	Daniel Kocher\\
    \affaddr{University of Salzburg}\\
    \email{Daniel.Kocher@stud.sbg.ac.at}
}

\maketitle

\begin{abstract}
This paper serves as a survey on \textit{ordered binary Decision diagrams}
(\textit{OBDDs}). It provides an overview of the OBDD representation, the
operations which can be applied efficiently on OBDDs and some aspects when it
comes to implementing OBDD packages. Moreover, their limitations and their
application in symbolic model checking are outlined and alternative representations
are presented to tackle these limitations. As this is just a survery, no new
insights are provided whatsoever.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

In computer-aided design (CAD) as well as other domains, like artificial
intelligence or combinatorics, many problems can be formulated as Boolean
functions. These Boolean functions then can be represented symbolically using
\textit{Binary Decision Diagrams} (\textit{BDDs}), introduced by Lee~\cite{LEE59}
and Akers~\cite{AKERS78} in 1959 and 1978, respectively.

In 1986, Bryant~\cite{BRYANT86} published a paper having strong impact, describing
a \textit{reduced} and \textit{ordered} class of BDDs, so-called \textit{ROBDDs}
(\textit{Reduced Ordered} BDDs), but in literature mostly referred to as
\textit{OBDDs}. OBDDs represent Boolean functions in a \textit{canonical}
form. This results in a compact form and very efficient tests for some properties,
e.g. satisfiability or equivalence~\cite{BRYANT86}.

Bryant also introduced and experimentally evaluated efficient implementation
techniques for OBDDs in collaboration with Brace and
Rudell~\cite{BRACE90, BRYANT92}.

Already in the inital paper describing OBDDs, the importance of the variable
ordering was highlighted. Only if the variable ordering is chosen properly, it
will result in a small graph (OBDD) and, in turn, in more efficient testing.

Representing Boolean functions using OBDDs has one drawback. Classes of
Boolean functions exist for which the size of the graph grows exponentially
independent of the variable ordering, e.g. integer
multiplication~\cite{BRYANT86, BRYANT91, WOELFEL01}.

These limitations of OBDDs motivated researchers to introduce heuristics and
other techniques like \textit{dynamic variable ordering}~\cite{RUDELL93} to solve
the problem of variable ordering~\cite{BOLLIG96}. Furthermore, alternative
representations were developed which may be beneficial for some classes of Boolean
functions but mostly suffer from losing the canonical form and, hence, the desirable
properties OBDDs are famous for.

This paper serves as survey on symbolic Boolean function manipulation using OBDDs.
The following section describes the representation of Boolean functions through
BDDs as well as OBDDs. It also highlights the advantages and disadvantages of the
OBDD representation. The third section describes operations which can be applied
to OBDDs efficiently. Section~\ref{sec:implementational-aspects} describes aspects
one has to take into account when implementing an OBDD package, such as what data
structures to use or how to deal with the problem of variable ordering.
In the subsequent sections, limitations of the OBDD representation and their usage
in symbolic model checking are outlined. The last section discusses alternative
representations to probably overcome the limitations mentioned before.

\section{Representation Basics}
\label{sec:representation-basics}

\textit{Binary decision diagrams} (\textit{BDDs}) have been well studied since
Lee~\cite{LEE59} and especially Akers~\cite{AKERS78} introduced them. BDDs
represent a Boolean function $f$ as an acyclic, directed graph (DAG). The function
arguments may be omitted in some cases to keep the formulas simple, i.e. $f$
denotes $f(x_1, \ldots, x_2)$. Nodes represent variables and have one incoming
and two outgoing edges, each of which denotes the variable evaluates to 0 (left)
or 1 (right), respectively. Whenever the result of $f$ is fixed, an edge points
to one of the constant functions \textbf{0} or \textbf{1}. To evaluate $f$ for a
given variable assignment, one just follows the path for the respective variable
values. As soon as the path ends up in a constant, i.e. \textbf{0} or \textbf{1},
this is the result of the corresponding variable assignment. Akers~\cite{AKERS78}
already provided some techniques to simplify the BDD representation such as
combining equivalent nodes.

Figure~\ref{subfig:bdd} shows an example of a BDD representing the Boolean function
$f=(A \land B) \lor (B \land C)$. To simplify the graphs, the diagram style of
Bryant~\cite{BRYANT92} is reused, i.e. edge labels are skipped. Instead dashed
and solid lines are used to represent the value of the decision variable to be 0
and 1, respectively. This diagram style is used throughout this paper.

\begin{figure}[ht]
    \centering
    \subfigure[BDD]{
        \includestandalone[height = 3cm]{figs/bdd}
        \label{subfig:bdd}
    }
    \hskip1cm
    \subfigure[OBDD]{
        \includestandalone[height = 3cm]{figs/obdd}
        \label{subfig:obdd}
    }
    \caption{BDD and OBDD of $f=(A \land B) \lor (B \land C)$.}
    \label{fig:bdd-and-obdd}
\end{figure}

However, Bryant~\cite{BRYANT86} introduced some restrictions concerning the
ordering of the so-called \textit{decision variables} represented by the nodes
of the BDD. Furthermore, Bryant\cite{BRYANT86} was able to derive a reduced
representation of BDDs providing the DAG in a canonical form: the
\textit{Reduced Ordered Binary Decision Diagram} (\textit{ROBDD}; mostly referred
to as \textit{OBDD}, even by Bryant himself). In this paper, the terms ROBDD and
OBDD are used synonymously.

Figure~\ref{subfig:obdd} shows one possible ROBDD representation of
$f$. The following section covers the details of how
to derive the OBDD representation from a BDD. \tbr

\subsection{From BDDs to OBDDs}
\label{subsec:from-bdds-to-obdds}

As the name suggests, \textit{Reduced} Ordered Binary Decision Diagrams are a 
minimized representation of BDDs. Furthermore, the decision variables are ordered
consistently. More formally, for any node $n$ and their respective children $m_1$
and $m_2$, a total ordering is imposed, i.e. $var(n) < var(m_1)$ and
$var(n) < var(m_2)$, respectively, where $var(x)$ denotes the label of node
$x$~\cite{BRYANT86, BRYANT92}.

This \textit{variable ordering} affects the efficiency of the symbolic
manipulation because different variable orderings, in general, produce different
OBDDs. In the worst case, this will result in exponential complexity for bad
orderings. Nevertheless, as will be described in Section~\ref{sec:limitations},
there exist classes of functions for which the OBDD complexity is exponential
regardless of which variable ordering is chosen.

Before discussing the reduction procedure, some definitions are necessary. Here
the original definitions of Bryant~\cite{BRYANT86, BRYANT91} are reused.

A Boolean function can be represented as an OBDD, i.e. a rooted, directed,
acyclic graph (DAG) with a set of nodes $V$ as well as a set of edges $E$. 

There exist \textit{nonterminal} and \textit{terminal} nodes. Each
\textit{nonterminal} node $n$ has its own label $var(n)$ as well as two edges,
connecting it to its left $lo(n)\in V$ and right child $hi(n)\in V$, respectively. 
Each \textit{terminal} node $t$ has a unique value $value(t)$, i.e. $value(t)$
is a constant function (for Boolean functions \textbf{0} or \textbf{1}). Hence,
for each constant function only a single terminal node exists.

The following property has to hold for any nonterminal node $n$ in order to
satisfy the ordering requirement of an OBDD:
$var(n) < var(lo(n)) \land var(n) < var(hi(n))$.
In other words, for any path starting at the root of an OBDD, if a path is
followed until a terminal node is reached, the nonterminal nodes are traversed
strictly in the variable ordering.

\paragraph*{Reduction procedure}
\mbox{} % workaround to get line break after paragraph

Bryant~\cite{BRYANT86, BRYANT92} described three transformation rules to reduce
a graph such that it still represents the same function as before.

For each constant function, there exists only a single terminal node representing
it, e.g. for Boolean functions, only two terminal nodes exist, representing
\textbf{0} and \textbf{1}, respectively. Hence, when transforming a given BDD,
eliminate all but one terminal node for each constant function and redirect all
edges accordingly, i.e. to the one remaining node for each constant function.

There may be duplicate nonterminal nodes present in a given BDD. Formally, a node
$d$ is a duplicate if $var(d) = var(n) \land lo(d) = lo(n) \land hi(d) = hi(n)$
holds for any other node $n$. If this is the case, $d$ can be eliminated and all
edges are redirected to $n$.

The third and last transformation rule deals with completely redundant nonterminal
nodes. Intuitively, this rule should be straightforward: any node having the same
child on all outgoing edges is redundant because the actual value of the variable
does not change the result of the function. More formally, a nonterminal node $n$
is redundant if and only if $lo(n) = hi(n)$. Then, all incoming edges of $n$ can
be redirected to $lo(n)$ and $n$ can be deleted afterwards.

\begin{figure}[ht]
    \centering
    \subfigure[]{
        \includestandalone[height = 3cm]{figs/redundant-nonterminals-before}
        \label{subfig:redundant-nonterminals-before}
    }
    \hskip1cm
    \subfigure[]{
        \includestandalone[height = 3cm]{figs/redundant-nonterminals-after}
        \label{subfig:redundant-nonterminals-after}
    }
    \caption{Eliminating redundant nonterminal nodes.}
    \label{fig:redundant-nonterminals}
\end{figure}

Figure~\ref{fig:bdd-and-obdd} shows the result, \ref{subfig:obdd}, of applying
the first two transformation rules to the BDD shown in \ref{subfig:bdd}.
The third rule is shown in Figure~\ref{fig:redundant-nonterminals}:
\ref{subfig:redundant-nonterminals-before} shows a BDD with redundant
nonterminal nodes (both B's, in essence) and
\ref{subfig:redundant-nonterminals-after} shows the equivalent OBDD without
redundant nonterminals.

Bryant already formally described an algorithm to transform an arbitrary graph
into a reduced one~\cite[p. 683]{BRYANT86}. 

OBDDs have one big advantage over other representations: their representation is
\textit{canonical}. As a consequence, important tests can be performed on an OBDD
in constant time, i.e. satisfiability and tautology tests. Moreover, OBDDs enable
other tests, such as variable independence and functional equivalence tests, to
be performed efficiently. \tbr

\section{Operations}
\label{sec:operations}

Bryant's~\cite{BRYANT86, BRYANT92} main focus was to define algorithms performing
symbolic operations on graphs representing Boolean functions. The most basic
operation is \textit{Apply} which applys a given operation $op$, e.g. \texttt{AND},
to two given functions $f_1$ and $f_2$, and returns the result of $f_1\;op\;f_2$.
Another operation is \textit{Restrict}, restricting a given variable $x$ to a
given (constant) value $c$, denoted $f|_{x\leftarrow c}$. Furthermore, the
operations \textit{Compose} and \textit{Satisfy} can be defined as graph
algorithms.

The following paragraphs summarize those operations and their properties briefly.
Implementation details are covered in Section~\ref{sec:implementational-aspects}.

\paragraph*{Restrict}
\mbox{} % workaround to get line break after paragraph

Although \textit{Apply} is the core operation, the \textit{Restrict} operation is
discussed first, mainly because it is used within \textit{Apply}. Besides,
\textit{Restrict} is straightforward and easy to understand.

A variable $x_i$, argument of a function $f$, can be restricted to a given value
$c$, denoted by $f|_{x_i\leftarrow c}$, as follows.

Starting from the root $n$ of the OBDD representation of $f$ and traversing the
OBDD depth-first, every node is checked for $var(n) = x_i$. This is done recursively
on the given graph, generating the resultant graph This node represents
the variable to be restricted. Now, there are two cases for $c$ to distinguish:
\begin{enumerate}
    \item{$c=0$: The incoming edge of $n$ is redirected to $lo(n)$.}
    \item{$c=1$: The incoming edge of $n$ is redirected to $hi(n)$.}
\end{enumerate}

By applying the three transformation rules of the reduction procedure in each
step, \textit{Restrict} generates a reduced graph immediately~\cite{BRYANT92}.

This procedure has linear time complexity in the number of nodes, i.e. $O(|V|)$,
assuming an efficient implementation (see
Section~\ref{sec:implementational-aspects} for details)~\cite{BRYANT92}. 
\tbr

\paragraph*{Apply}
\mbox{} % workaround to get line break after paragraph

Applies an operation $op$ to two given functions $f_1$ and $f_2$ and returns the
resulting function $f_1\;op\;f_2$. This is the most intuitive function one would
expect when dealing with Boolean functions and it has some interesting properties
too, i.e. the complement of $f_1$ can be computed by setting $f_2$ to \textbf{1}
and $op$ to $\oplus$ (\texttt{XOR}). Moreover, the already mentioned functional
equivalence can be tested by applying $\overline{\oplus}$ (\texttt{XNOR}) to
$f_1$ and $f_2$ for all relevant cases. Therefore, Bryant~\cite{BRYANT92} defined
a function $d(x_1, \ldots, x_n)$ for \textit{Don't cares}:

$d(x_1, \ldots, x_n)=
\begin{cases}
    1, & \text{if values are not relevant} \\
    0, & otherwise
\end{cases}
$

\noindent
Using this function, one check if $f_1$ and $f_2$ are functionally equivalent by
computing $(f_1\overline{\oplus}f_2)\lor d$. If the result is \textbf{1}, $f_1$
and $f_2$ are functionally equivalent.

\textit{Apply} traverses the given graph depth-first, creating the resultant OBDD
step by step. Starting from the roots $n_1$ and $n_2$, new nodes are created for
each branching point of the graphs of $f_1$ and $f_2$, respectively.

\textit{Shannon's expansion} forms the basis of the recursive
algorithm~\cite{BRYANT86}:
\begin{center}
$
f_1 op f_2 =
\overline{x_i} \cdot \left(f_1|_{x_i\leftarrow 0}\;op\;f_2|_{x_i\leftarrow 0}\right)
+ x_i \cdot \left(f_1|_{x_i\leftarrow 1}\;op\;f_2|_{x_i\leftarrow 1}\right)
$
\end{center}

\noindent
The recursive procedure then has three different cases. Note that a node is only
created if no equivalent node is already present in the resultant graph.
\begin{enumerate}
    \item{
        If $n_1$ and $n_2$ are both terminal nodes, then a terminal node
        representing the value $value(n_1)\;op\;value(n_2)$ is added to the
        resultant graph and the recursion terminates.
    }
    \item{
        If either $n_1$ or $n_2$ is a nonterminal node, then:
        \begin{enumerate}
            \item{
                If $var(n_1) = var(n_2)$, a node having $var(n_1)$ is created and
                the procedure is applied recursively on $(lo(n_1), lo(n_2))$ and
                $(hi(n_1), hi(n_2))$.
            }
            \item{
                If $var(n_1)$ is a nonterminal and $n_2$ is eiter a terminal node
                or $var(n_1) < var(n_2)$, a node having $var(n_1)$ is created and
                the procedure is applied recursively on $(lo(n_1), n_2)$ and
                $(hi(n_1), n_2)$.
            }
        \end{enumerate}
    }
\end{enumerate}

To get a reduced graph immediately, the three transformation rules are utilized
again for each step~\cite{BRYANT92}.

There are some refinements when it comes to implementation. Those are covered in
Section~\ref{sec:implementational-aspects} as well as the original paper of
Bryant~\cite{BRYANT86, BRYANT92}.  However, assuming an efficient implementation,
the time complexity can be bound to
$O\left(|V_{f_1}| \cdot |V_{f_2}|\right)$~\cite{BRYANT86, BRYANT92}.

The full pseudocode defined by Bryant can be found in~\cite[p. 685]{BRYANT86}. \tbr

\paragraph*{Compose}
\mbox{} % workaround to get line break after paragraph

This operation is merely based upon calls to \textit{Apply} and \textit{Restrict}.
Utilizing \textit{Shannon's expansion}, Bryant~\cite{BRYANT86} formulated the
composition of two functions $f_1$ and $f_2$ as
\begin{center}
$
f_1|_{x_i=f_2}=
f_2 \cdot f_1|_{x_i=1} + \overline{f_2} \cdot f_1|_{x_i=0}
$
\end{center}

This would result in an algorithm having a worst-case time complexity of 
$O\left(|V_{f_1}|^2 \cdot |V_{f_2}|^2\right)$. Therefore, Bryant~\cite{BRYANT86}
already proposed to extend the \textit{Apply} operation to support ternary
operations and then use the equivalent \textit{if-then-else} ($ITE$)
representation:
\begin{center}
$
ITE\left(f_2, f_1|_{x_i=1}, f_1|_{x_i=0}\right) =
f_2 \cdot f_1|_{x_i=1} + \overline{f_2} \cdot f_1|_{x_i=0}
$
\end{center}

\noindent
For a given node $x$, the $ITE$ operator for three nodes, \newline
$ITE(x, hi(x), lo(x))$, chooses $hi(x)$ if $x$ evaluates to true and $lo(x)$
otherwise. Computing $ITE(x, hi(x), lo(x))$ is accomplished by utilizing a
recursive formulation~\cite{BRACE90}. To shorten the recursive formula, $l$ and
$h$ are used to represent $lo(x)$ and $hi(x)$, respectively.
\begin{center}
$
ITE(x, h, l) =
(var(x), ITE(x|_{x=1}, h|_{x=1}, l|_{x=1}), ITE(x|_{x=0}, h|_{x=0}, l|_{x=0}))
$
\end{center}

This formulation is based upon the fact, that every node $n$ of an OBDD represents
a Boolean function and is denoted by a triple $(var(n), hi(n), lo(n))$. Brace et
al.~\cite{BRACE90} called the variable associated with $n$, i.e. $x_n$ the
\textit{top variable} of $n$.

The time complexity then improves to be
$O\left(|V_{f_1}|^2 \cdot |V_{f_2}|\right)$~\cite{BRYANT86} when using the $ITE$
operator.~\cite[p. 686]{BRYANT86} lists Bryant's full pseudocode.

Bryant~\cite{BRYANT86} also described another, even more efficient way to compose
two function graphs. But this approach only applicable under restricted conditions.
\tbr

\paragraph*{Satisfy}
\mbox{} % workaround to get line break after paragraph

In essence, Bryant~\cite{BRYANT86} described two operations, i.e.
\textit{Satisfy-one} and \textit{Satisfy-all}, both concerned with the satisfying
set. As the name suggests, \textit{Satisfy-one} selects a single element and
\textit{Satisfy-all} selects all elements from the satisfying set.

\textit{Satisfy-one} traverses the OBDD depth-first and uses backtracking until
a terminal node of the constant function \textbf{1} is encountered. If such a
node is found, the corresponding Boolean function is satisfiable and true is
returned. Furthermore, an array representing the satisfying variable assignment
is assigned. The time complexity for a reduced graph is linear, i.e.
$O(|V_f|)$~\cite{BRYANT86}.

\textit{Satisfy-all} also traverses the OBDD depth-first. Every time the terminal
node having value \textbf{1} is traversed, it is printed. The recursive procedure
terminates as soon as the terminal node having value \textbf{0} is reached. This
results in a time complexity of $O(|V_f| \cdot |S_f|)$, whereas $S_f$ denotes the
satisfying set~\cite{BRYANT86}. \tbr

\section{Implementational Aspects}
\label{sec:implementational-aspects}

Bryant~\cite{BRYANT86} already observed the importance of utilizing efficient
techniques to implement the operations. Computation applied to OBDDs has a highly
dynamic character without specific memory access patterns~\cite{BRYANT92}.

Hence, the following sections cover important aspects to keep in mind when
implementing an OBDD framework efficiently, i.e. efficient data structures, the
$ITE$ operator, how to possibly approach the variable ordering problem and
\textit{garbage collection}. \tbr

\subsection{Efficient Data Structures}
\label{subsec:efficient-data-structures}

\paragraph*{Hash Tables}
\mbox{} % workaround to get line break after paragraph

Hash tables maintain \textit{key}-\textit{value}-pairs, returning the associated
value for a given key in (amortized) constant time. Collisions, i.e. equivalent
value for different keys, are resolved by chaining all associated values in a 
linked list~\cite{BRACE90}.

This is especially useful to refine \textit{Apply} in order to guarantee the time
complexity of $O\left(|V_{f_1}| \cdot |V_{f_2}|\right)$. Using a hash table to
match pairs of nodes ($n_1$, $n_2$) to a single node $m$, \textit{Apply} avoids
making multiple recursive calls to equivalent pairs of nodes and thus avoids
computing equivalent subgraphs multiple times. Therefore, before computing a
subgraph, \textit{Apply} checks whether an entry is present for the given pair
of nodes. If so, the associated value $m$ is used instead of computing the
subgraph~\cite{BRYANT86, BRYANT92}.

Moreover, hash tables are useful to generate a reduced graph directly from some
operations by maintaing a hash table associating triples
($var(n)$, $lo(n)$, $hi(n)$) with a single nonterminal node $n$. This is done for
each nonterminal node $n$ generated while executing the
operation~\cite{BRYANT92}. This is also referred to as
\textit{unique-table}~\cite{BRACE90} imposes the canonical form of an OBDD. If a
new node is generated, a corresponding entry is created in the unique-table.

There exists a special version of hash tables, so-called
\textit{hash-based caches}, which do not resolve collisions, i.e. overwrite
existing entries, and issue a cache miss if a looked up element is not found.
As will be seen, \textit{Hash-based caches} are useful to provide an efficient
implementation of the $ITE$ operator. \tbr

\paragraph*{Graph Representation}
\mbox{} % workaround to get line break after paragraph

To reduce the number of nodes of an OBDD, one can use \textit{complement edges}.
A complement edge is an edge holding a single bit to indicate complementation.
Hence, when $\overline{f}$ needs to be represented and a node $f$ is already
present, a complement edge can be used to connect $f$ and thus avoid
constructing the intermediate node $\overline{f}$. Also one of the two terminal
nodes (e.g. \textbf{1}) can be removed by just using a complement edge to connect
the other one (e.g. \textbf{0})~\cite{BRACE90}.

Brace et al.~\cite{BRACE90} identified four functionally equivalent pairs of
functions to be used for complement edges in order to maintain the canonical form.
Let $x$ be the node having $lo(x)$ and $hi(x)$ on its outgoing 0- and 1-edge,
respectively. Moreover, let $in(x)$ be the (one) incoming edge of node $x$ and
$\overline{c}$ the complement of $c$. Then, the following table sums up the
four pairs identified by Brace et al.~\cite{BRACE90}.

\begin{center}
    \begin{tabular}{lll}
        $\left(in(x), lo(x), hi(x)\right)$ & $\Leftrightarrow$ & $\left(\overline{in(x)}, \overline{lo(x)}, \overline{hi(x)}\right)$ \tabularnewline
        $\left(\overline{in(x)}, lo(x), hi(x)\right)$ & $\Leftrightarrow$ & $\left(in(x), \overline{lo(x)}, \overline{hi(x)}\right)$ \tabularnewline
        $\left(in(x), \overline{lo(x)}, hi(x)\right)$ & $\Leftrightarrow$ & $\left(\overline{in(x)}, lo(x), \overline{hi(x)}\right)$ \tabularnewline
        $\left(\overline{in(x)}, \overline{lo(x)}, hi(x)\right)$ & $\Leftrightarrow$ & $\left(in(x), lo(x), \overline{hi(x)}\right)$ \tabularnewline
    \end{tabular}
\end{center}

Also, the 1-edge of each node must always be a regular one to guarantee a
canonical form.

Implementing complement edges enables one to compute the complement and identify
complement functions in constant time (because $f$ and $\overline{f}$ use the same
node). According to Brace et al.~\cite{BRACE90} this reduces the total runtime
to form the OBBDD by almost a factor of 2 (for 12 examples).

Another refinement to the graph representation, also proposed by Brace et
al.~\cite{BRACE90}, is to integrate the unique-table into the graph representation,
i.e. linking the collision chain of each node to the corresponding unique-table
entry, to improve memory usage. \tbr

\subsection{ITE Operator}
\label{subsec:ite-operator}

Accoring to Brace et al.~\cite{BRACE90}, the $ITE$ operator forms the core of an
efficient OBDD package implementation. Hence, they used a hash-based cache, called
\textit{caching computed-table}, to implement it, exploiting a high locality of
reference and consuming less memory.

Compared to using a normal hash table, this results in more recursive calls to 
$ITE$ (because of cache misses) but less time spent in garbage collection
(described in Section~\ref{subsec:garbage-collection}) and look-ups (because no
collision chains need to be searched).

Nevertheless, it is worth noting that the worst-case time complexity increases
to be exponential if all keys are hashed to the same value, compared to a
polynomial time complexity for normal hash tables~\cite{BRACE90}. 

Furthermore, Brace et al.~\cite{BRACE90} advise to use \textit{standard triples},
i.e. to define an $ITE(f_1, f_2, f_3)$-based equivalence relation on sets of
three functions $f_1$, $f_2$ and $f_3$. This takes advantage of the fact that
different parameters may yield the same result when $ITE$ is called. Hence, before
the computed-table is accessed when executing $ITE$, the arguments are replaced
by the standard arguments. Using this approach some recomputations can be avoided
and, obviously, less space is consumed by the computed-table~\cite{BRACE90}.

Brace et al.~\cite[p. 42]{BRACE90} provides the pseudocode of the $ITE$ algorithm
as well as a modification, called $ite\_constant$, which can be used to implement
logical implication tests more efficiently by avoiding the construction of
intermediate nodes. \tbr

\subsection{Garbage Collection}
\label{subsec:garbage-collection}

The first OBDD package containing kind of a garbage collection was the one
introduced by Brace, Bryant and Rudell~\cite{BRACE90}. The garbage collection was
based upon the reference counting technique, i.e. every node of the OBDD had a
reference count. This reference count represents two kinds of references, (1)
the number of other nodes and (2) the number of formulas referencing it, but does
not take references from the unique- and computed-table into account.

Today, garbage collecion is a well-known topic. It is used to free up unnecessary
allocated or unused memory. To accomplish this, the garbage collection has to know
about which parts of the memory are \textit{dead} and thus can be freed. A node
of an OBDD is called \textit{dead} if the reference count of it is equal to zero.
The reference count of a node $x$ is decremented every time a corresponding
formula is freed. If the reference count of $x$ reaches zero, the reference counts
of $lo(x)$ and $hi(x)$ are decremented recursively~\cite{BRACE90}.

Instead of freeing dead nodes instantaneously, there exists a \textit{reclaim}
operation which is performed whenever a lookup in the computed-table returns a
dead node $x$. Such a reclaim operation increments the reference count of $x$ and
propagates the reclaim operation to $lo(x)$ and $hi(x)$,
respectively~\cite{BRACE90}.

The garbage collection is executed whenever the load factor (the ratio between
number of keys stored and number of available bins) exceeds 4 after an insertion
and $10\%$ of all OBDD nodes are dead. A garbage collection run deletes all entries
in the computed-table referencing dead nodes, and frees all dead nodes. The sizes
of computed- as well as unique-table are increased if the garbage collection does
not find enough dead nodes. Another event invoking the garbage collection is the
memory overflow of computed- and unique-table, although, the $10\%$-threshold of
dead nodes is used again. In any other case, the garbage collection gives up (and,
of course, frees all intermediate nodes)~\cite{BRACE90}.

As stated by Brace et al.~\cite{BRACE90}, this provides effective memory usage
at very low run-time costs, i.e. $3-7\%$ of the total runtime. \tbr

\subsection{Dynamic Variable Ordering}
\label{subsec:dynamic-variable-ordering}

For an OBDD package it is desirable to provide the possibility to automatically 
choose a good, though not optimal variable order for arbitrary functions. Of
course, there exist heuristics to order variables before processing but
Rudell~\cite{RUDELL93} proposed to integrate the determination and maintainance
of the variable order into the OBDD package itself. This approach to the variable
ordering problem is called \textit{dynamic variable ordering} and increases the
robustness of an OBDD package by removing the necessity of choosing a proper
variable ordering beforehand and thus running the risk of spacing out.

Dynamic variable ordering executes the \textit{sifting} algorithm, also proposed
by Rudell~\cite{RUDELL93}, to reorder the variables periodically and so minimize
the size of the OBDD. This on-the-fly approach enabled Rudell~\cite{RUDELL93} to
overcome hard problems for which heuristics did not find a variable ordering good
enough to actually solve them, i.e. heuristics were able to solve 24 out of 35
large circuits from the IWLS'91 and dynamic variable ordering using the sift
algorithm was able to solve 33 out of these 35 large circuits~\cite{RUDELL93}.

Periodically means, that a reordering is not performed by user requests but at
appropriate points, determined by the OBDD package itself and so eliminate any
user responsibility.

The core forms an efficient variable swap implementation, i.e. keep the changes
in the OBDD as local as possible when swapping two adjacent variables $x$ and
$y$. It has been found that swapping variable of two adjacent levels $i$ and
$(i+1)$ only affects the nodes of these two levels. Hence, Rudell~\cite{RUDELL93}
proposed to keep a separate unique-table for each level of the OBDD, similar to
the last refinement in \textit{Graph representation} of
Section~\ref{subsec:efficient-data-structures}. In essence, keeping separate 
unique-tables for each node is the best solution.

To ensure that each node represents the same function before and after the variable
swap, Rudell~\cite{RUDELL93} proposed a node overwriting strategy. A given node
$(var(x_i), hi(x_i), lo(x_i))$ at level $i$ is overwritten by the node triple
\begin{center}
$(var(x_{i+1}), (x_i, h|_{x=1}, l|_{x=1}), (x_i, h|_{x=0}, l|_{x=0}))$
\end{center}
Here the notation of \textit{Compose} (Section~\ref{sec:operations}) is reused
to simplify the formula, i.e. $h$ ($l$) denotes $hi(x_i)$ ($lo(x_i)$).

Rudell~\cite{RUDELL93} also proposed to clear the computed-table and issue a
garbage collection before minimizing the OBDD. Afterwards, all nodes of level
$(i+1)$ can be freed incrementally if there exist no references from elsewhere
than level $i$.

To minimize the OBDD, a novel minimization algorithm was introduced by
Rudell~\cite{RUDELL93}, referred to as \textit{sifting algorithm}. It tries to
find the ideal position of one variable when all other variables keep their
positions. By definition, the ideal position of a variable is the position which
minimizes the size of the OBDD.

The sifting algorithm therefore uses a brute-force enumeration strategy to find
the optimal variable position. It starts by swapping the processed variable $x$
downwards one level at a time, i.e. swapping it with its successor variable, which
is located on the layer below in the OBDD. This is done until $x$ is the next to
last variable in the OBDD. Then, this is done in the other way round, i.e. swapping
$x$ with its predecessor variable until $x$ becomes the top variable of the OBDD.
While proceeding, the ideal position is memorized and after $x$ became the top
variable, $x$ is moved to this ideal position by, again, swapping levels one by
one~\cite{RUDELL93}. Figure~\ref{fig:sifting-algorithm} shows this procedure
applied to an example, requiring a total of 9 steps for 5 possible positions. The
first phase, determining the ideal position by swapping two variables at a time,
is marked red, i.e. the two swapped variables are marked. The variable pairs
swapped during the second phase, i.e. moving the $x$ to its ideal position (4),
are marked green. 

\begin{figure}
    \centering
    \begin{tabular}{l|l|l|l|l|l|}
        \hline
        \diag{0.1em}{1cm}{\scriptsize{Step}}{\scriptsize{Pos}} & $1$ & $2$ & $3$ & $4$ & $5$ \tabularnewline
        \hline
        init & $v$ & $w$ & $x$ & $y$ & $z$ \tabularnewline
        \hline
        1 & $v$ & $w$ & \cellcolor{red!50}$y$ & \cellcolor{red!50}$x$ & $z$ \tabularnewline
        \hline
        2 & $v$ & $w$ & $y$ & \cellcolor{red!50}$z$ & \cellcolor{red!50}$x$ \tabularnewline
        \hline
        3 & $v$ & $w$ & $y$ & \cellcolor{red!50}$x$ & \cellcolor{red!50}$z$ \tabularnewline
        \hline
        4 & $v$ & $w$ & \cellcolor{red!50}$x$ & \cellcolor{red!50}$y$ & $z$ \tabularnewline
        \hline
        5 & $v$ & \cellcolor{red!50}$x$ & \cellcolor{red!50}$w$ & $y$ & $z$ \tabularnewline
        \hline
        6 & \cellcolor{red!50}$x$ & \cellcolor{red!50}$v$ & $w$ & $y$ & $z$ \tabularnewline
        \hline\hline
        7 & \cellcolor{green!50}$v$ & \cellcolor{green!50}$x$ & $w$ & $y$ & $z$ \tabularnewline
        \hline
        8 & $v$ & \cellcolor{green!50}$w$ & \cellcolor{green!50}$x$ & $y$ & $z$ \tabularnewline
        \hline
        9 & $v$ & $w$ & \cellcolor{green!50}$y$ & \cellcolor{green!50}$x$ & $z$ \tabularnewline
        \hline
    \end{tabular}
    \caption{Sifting algorithm (ideal position of $x$: 4)}
    \label{fig:sifting-algorithm}
\end{figure}

This procedure could be refined to choose the direction before actually start
swapping the processed variable. As a result, the variable is always swapped
towards the closer variable position first.

To obtain a proper variable ordering, variables are sorted descending in the number
of nodes per OBDD level. Then, the ideal position is computed for each variable
utilizing the approach described above. The sifting algorithm has one big advantage
over other similar algorithms, like \textit{window permutation algorithm}, namely
variables can move many levels because all positions are evaluated, regardless of
the temporary size of the OBDD~\cite{RUDELL93}.

According to Rudell~\cite{RUDELL93}, $O(n^2)$ swaps of adjacent levels are
necessary. Moreover, each swap has a complexity proportional to the with of the
OBDD. Thus, Rudell~\cite{RUDELL93} advises to terminate whenever the OBDD doubles
its size. On the one hand, in contrast to heuristics and similar approaches, i.e.
the window permutation algorithm, the dynamic variable ordering using the sifting
algorithm has a significantly higher runtime. On the other hand, dynamic variable
ordering using the sifting algorithm was able to complete operations which were
not completed without it. Furthermore, the resulting OBDDs were significantly
smaller~\cite{RUDELL93}.

Unfortunately, dynamic variable ordering has some bad consequences as well, namely
\textit{negate-input-} and \textit{negate-else-edges} cannot be used without
loosing the (desired) local complexity of variable swapping.

However, dynamic variable ordering is a good practice to obtain minimized OBDDs.
For example, Figure~\ref{fig:min-obdd} shows the minimized, and thus optimal, OBDD
representation of \newline $f=\left(A \land B\right) \lor \left(B \land C\right)$
which, compared to the OBDD shown in Figure~\ref{subfig:obdd}, consists of less
nodes.

\begin{figure}[ht]
    \centering
    \includestandalone[height = 3cm]{figs/min-obdd}
    \caption{Minimized OBDD (optimal).}
    \label{fig:min-obdd}
\end{figure}
\tbr

\section{Limitations}
\label{sec:limitations}

Some limitation were already mentioned briefly. This section provides a deeper
insight on what cannot be accomplished using the standard OBDD representation.
This includes classes of Boolean function which cannot be computed efficiently
using OBDDs and, again, the variable ordering problem.

Already in 1986, Bryant~\cite{BRYANT86} noticed that some classes of Boolean
functions cannot be represented by an OBDD efficiently, regardless of the variable
ordering chosen. He then studied the complexity of Boolean functions in
1991~\cite{BRYANT91} to provide a lower bound on the OBDD as data structure (and
very large scale integrated circuits). The complexity was expressed in the minimum
size of the OBDD, i.e. the minimal number of nodes for any possible variable
ordering $\pi$. Bryant~\cite{BRYANT91} mainly focused on the set of integer
multipliers having word size $n$.

Bryant~\cite{BRYANT91} first generalized a function over $2n$ variables. The size
of an OBDD then can range from $2n$ up to $2^{n+1}-2$ for different variable
orderings. Now, analyzing $n$-bit integer multiplication, Bryant~\cite{BRYANT86}
already proved that for any variable ordering, there exists an $0 \le i \le 2n$
such that the function representing output $i$ requires an OBDD of size
$\Omega(1.09^n)$. Bryant himself~\cite{BRYANT91} showed that there is not much
of a difference between computing the single output $(n-1)$ or the entire product
of an $n$-bit integer multiplier. Bryant~\cite{BRYANT91} found the $n$-bit integer
multiplication to be especially hard, requiring an OBDD having at least $2^{n/8}$
nodes to compute the middle bit of the built product for any variable ordering.
Hence, the complexity of an OBDD representing integer multiplication is
exponential in $n$, where $n$ denotes the word size of the multiplied integers.
One can read the full proof in~\cite{BRYANT91}.

Nevertheless, researches put some effort in improving this lower bound. In 2001,
Woelfel~\cite{WOELFEL01} published a new bound lower bound as well as a first 
nontrivial upper bound for the size of OBDDs representing $n$-bit integer
multiplication by utilizing the concept of \textit{universal hashing}. Therefore,
he utilized a universal family of hash functions to improve the exponential lower
bound on the OBDD-size of integer multiplication to
$2^{\lfloor\frac{n}{2}\rfloor}/96$. Moreover, Woelfel~\cite{WOELFEL01} proved
the upper bound for the OBDD-size of computing the middle bit of an $n$-bit
multiplication to be $7/3 \cdot 2^{\frac{4 \cdot n}{3}}$.

Nevertheless, all lower bounds for $n$-bit integer multiplication are exponential
in the number of nodes. Especially because of the high practical relevance of
$n$-bit integer multiplication this is one of the main drawbacks of the OBDD
representation~\cite{BRYANT86, BRYANT91, WOELFEL01}.

Another topic researchers put much effort into was the variable ordering problem.
Section~\ref{subsec:dynamic-variable-ordering} already described one possible
approach to this problem but there are many other approaches such as heuristics
and simulated annealing algorithms~\cite{BOLLIG96}.

Bollig and Wegener~\cite{BOLLIG96} studied the complexity of improving the
variable ordering. They investigated this problem by analyzing the following
problem: Given an OBDD representing $f$ and a size bound $s$. Does an OBDD exist,
also representing $f$ and respecting an arbitrary variable ordering, having at
most $s$ nodes? "Unfortunately", they were able to prove this problem to be 
NP-complete. Hence, improving the variable ordering of OBDDs is
NP-complete~\cite{BOLLIG96}. \tbr

\section{Symbolic Model Checking}
\label{sec:symbolic-model-checking}

One specific application of reduced ordered binary decision diagrams is 
\textit{symbolic model checking}, introduced by Burch et al.~\cite{BURCH90} in
1990. \textit{Symbolic}, because the state space is not represented explicitly
but symbolically by utilizing OBDDs to represent $\mu$-Calculus formulas.
In~\cite{BURCH90}, Burch, Clarke, McMillan, Dill and Hwang derived an efficient
decision procedures for CTL model checking as well as satisfiability of LTL
formulas, strong and weak observational equivalence of finite transition systems
and language containment for finite $\omega$-automata by introducing a new
$\mu$-Calculus model checking algorithm. However, this survey just outlines the
$\mu$-Calculus and the efficient decision procedures for CTL model checking.

The main idea behind this approach is to represent the state space symbolically,
instead of explicitly with e.g. a state table. Such explicit state tables may grow
exponentially with the number of concurrent components, because the state space
does so. Commonly, this is referred to as the
\textit{state explosion problem}~\cite{BURCH90, MCMILLAN92}. To overcome this
problem, Burch et al.~\cite{BURCH90} as well as McMillan~\cite{MCMILLAN92}
proposed to utilize reduced ordered binary decision diagrams to represent the 
state space symbolically. Although OBDDs cannot avoid a state explosion in general,
many practically relevant systemes with large state spaces can be
verified~\cite{BURCH90, MCMILLAN92}. Experiments~\cite{BURCH90} showed that
symbolic model checking enables to verify models with more than $10^{20}$ states,
whereas explicit enumeration approaches can only verify models up to $10^6$
states. 

Burch et al.~\cite{BURCH90} and McMillan~\cite{MCMILLAN92} extend Bryant's OBDD
representation~\cite{BRYANT86} by quantification over Boolean variables (QBF) and
substitution of variable names. The quantification over Boolean variables is
realized by using Bryant's \textit{Restrict} operation (see
Section~\ref{sec:operations}). Let $f$ be a QBF formula and $x$ be a Boolean
variable. The QBF formula $\exists x\left(f\right)$ then is equivalent to
$f|_{a=0} \lor f|_{a=1}$~\cite{BURCH90}. Substitution of variables, e.g.
substitution of $x$ by $y$ (denoted $f\langle x\leftarrow y\rangle$), is computed
using the following equivalence:
\begin{center}
$
f\langle x\leftarrow y\rangle \equiv
\exists x\lbrack\left(x \Leftrightarrow y\right) \land f\rbrack
$
\end{center}

Before discussing the symbolic model checking algorithms $\text{BDD}_f$ and
$\text{BDD}_R$, the used $\mu$-Calculus has to be described. For a given finite
signature $\mathcal{S}$, each symbol is either an individual or a predicate
variable having positive arity. Burch et al.~\cite{BURCH90} considered two
syntactic categories, i.e. \textit{formulas} and \textit{relational terms}, having
the following forms.

\noindent
\textbf{Formulas:}~\cite{BURCH90}
\begin{enumerate}
    \item{\textbf{True}, \textbf{False}}.
    \item{
        $\lbrack z_1 = z_2 \rbrack$; $z_{1, 2}\in\mathcal{S}$ are individual
        variables.
    }
    \item{
        $\neg f$, $f \lor g$, $\exists z\left( f \right)$; $z\in\mathcal{S}$ is
        an individual variable, $f$ and $g$ are formulas.
    }
    \item{
        $P\left(z_1, \ldots, z_n\right)$; $P$ is an $n$-ary relational term and
        \newline $z_{1, \ldots, n}\in\mathcal{S}$ are individual variables not
        free in $P$.
    }
\end{enumerate}
\textbf{$n$-ary relational terms:}~\cite{BURCH90}
\begin{enumerate}
    \item{$Z$; $Z\in\mathcal{S}$ is an $n$-ary predicate variable.}
    \item{
        $\lambda z_1, \ldots, z_n\left( f \right)$; $f$ is a formula and
        $z_1, \ldots, z_n\in\mathcal{S}$ are distinct individual variables.
    }
    \item{
        $\mu Z \left( P \right)$; $Z\in\mathcal{S}$ is an $n$-ary predicate variable
        and $P$ is an $n$-ary relational term \textit{formally monotone in} $Z$,
        i.e. all free occurrences of $Z$ in $P$ have to be evenly negated. This
        $n$-ary relatioinal term is the \textit{least fixed point} (\textit{lfp})
        of an $n$-ary relational term $P$. Analogous, $\nu Z \left( P \right)$ is
        the \textit{greatest fixed point} (\textit{gfp}) and is equivalent to
        $\neg\mu Z \left( \neg P \langle Z \leftarrow \left( \neg Z \right) \rangle \right)$.
    }
\end{enumerate}

To determine whether a formula is \textbf{True} or \textbf{False}, Burch et
al.~\cite{BURCH90} defined a structure $\mathcal{M}\left( D, I_P, I_D \right)$,
where $D$, $I_P$ and $I_D$ represent the (non-empty) \textit{domain} of
$\mathcal{M}$, the \textit{relational variable interpretation} and the
\textit{individual variable interpretation}, respectively. Furthermore, a semantic
function $\mathcal{D}$ is introduced, mapping formulas to elements of
\begin{center}
$\left( I_P \rightarrow \left( I_D \rightarrow \lbrace true, false \rbrace \right) \right)$
\end{center}
and $n$-ary relational terms to elements of
\begin{center}
$\left( I_P \rightarrow \left( I_D \rightarrow 2^{D^n} \right) \right)$
\end{center}.

Burch et al.~\cite{BURCH90} then defined $\mathcal{D}$ inductively on formulas
and relational terms. For a structure $\mathcal{M}$ and a formula $f$,
$\mathcal{M}$ satisfies $f$, written as $\mathcal{M} \models f$, according to
these semantics:
\begin{enumerate}
    \item{
        $\mathcal{D} \left( z_1 = z_2 \right) \left( I_P \right) \left( I_D \right) :=
        \left( I_D \left( z_1 \right) = I_D \left( z_2 \right) \right)$
    }
    \item{
        $\mathcal{D} \left( \neg f_1 \right) \left( I_P \right) \left( I_D \right) :=
        \neg \left( \mathcal{D} \left( f_1 \right) \left( I_P \right) \left( I_D \right) \right)$
    }
    \item{
        $\mathcal{D} \left( f_1 \lor f_2 \right) \left( I_P \right) \left( I_D \right) := \newline
        \mathcal{D} \left( f_1 \right) \left( I_P \right) \left( I_D \right) \lor\;\mathcal{D} \left( f_2 \right) \left( I_P \right) \left( I_D \right)$
    }
    \item{
        $\mathcal{D} \left( \exists z \left( f_1 \right) \right) \left( I_P \right) \left( I_D \right) := \newline
        \exists e \in D . \left( \mathcal{D} \left( f_1 \right) \left( I_P \right) \left( I_D \langle z \leftarrow e \rangle \right) \right)$
    }
    \item{
        $\mathcal{D} \left( P\left( z_1, \ldots, z_n \right) \right) := \newline
        \langle I_D \left( z_1 \right), \ldots, I_D \left( z_n \right) \rangle \in \mathcal{D} \left( P \right) \left( I_P \right) \left( I_D \right)$
    }
    \item{
        $\mathcal{D} \left( Z \right) \left( I_P \right) \left( I_D \right) :=
        I_P \left( Z \right)$
    }
    \item{
        $\mathcal{D} \left( \lambda z_1, \ldots, z_n \left( f \right) \right) \left( I_P \right) \left( I_D \right) := \newline
        \lbrace \langle e_1, \ldots, e_n \rangle : \mathcal{D} \left( f \right) \left( I_P \right) \left( I_D \langle z_1 \leftarrow e_1, \ldots, z_n \leftarrow e_n \rangle \right) \rbrace$
    }
    \item{
        $\mathcal{D} \left( \mu Z \left( P \right) \right) :=
        \text{lfp}\;\lambda Q \in 2^{D^n} . \left( \mathcal{D} \left( P \right) \left( I_P \langle Z \leftarrow Q \rangle \right) \left( I_D \right) \right)$,
        where lfp is the least fixed point w.r.t. the inclusion ordering.
    }
\end{enumerate}

Finally, after defining the $\mu$-Calculus formally, the model checking algorithms
can be described. They operate on the limited Boolean domain, i.e.
$D = \lbrace 0, 1 \rbrace$ but Burch et al.~\cite{BURCH90} also described how any
\textit{finite} domain can be encoded into this limited Boolean domain. This last
fact generalizes the introduced algorithms to any \textit{finite} domain.

They described two function representing the algorithm:
$\text{BDD}_f \left( f, I_P \right)$ and $\text{BDD}_R \left( R, I_P \right)$,
where $f$ and $R$ denote a formula and a relational term, respectively. For 
$\text{BDD}_f$, $I_P$ represents a relational variable interpretation assigning
values to free relational variables in $f$~\cite{BURCH90}.

A given individual variable interpretation $I_D$ satisfies \newline
$\text{BDD}_f \left( f, I_P \right)$ iff $f$ is satisfied by $\mathcal{M}$ having
interpretations $I_P$ and $I_D$. OBDDs in combination with place-holder variables
$d_1, \ldots, d_n$ are used to represent the values of the relational variables
$\in I_P$. $d_i$ represents the $i^{th}$ argument of a relation and an $n$-ary
relation holds for arguments $y_1, \ldots, y_n$ iff the interpretation
$\langle d_1 \leftarrow y_1, \ldots, d_n \leftarrow y_n \rangle$ satisfies the
OBDD.

In essence, $\text{BDD}_f \left( f, I_P \right)$ distinguishes between five cases
derived from the OBDD semantics and the $\mu$-Calculus, utilizing the graph-based
algorithms of Bryant~\cite{BRYANT86} (also see Section~\ref{sec:operations}) as
well as the $\text{BDD}_R$ function (for applying a relational term $R$) with a
subsequent substitution of the place-holder variables with the argument variables, 
i.e. \newline
$\langle d_1 \leftarrow x_1, \ldots, d_n \leftarrow x_n \rangle$~\cite{BURCH90}.

The OBDD returned by $\text{BDD}_R \left( R, I_P \right)$ may contain both types
of variables, the place-holder and the individual variables, because a relational
term may contain free individual variables. Hence,
$\text{BDD}_R \left( R, I_P \right)$ is satisfied iff
$\mathcal{D} \left( R \right) \left( I_P \right) \left( I_D \right)$ holds for
the $n$-tuple $\langle I_A \left( d_1 \right), \ldots, I_A \left( d_n \right) \rangle$
for given $I_D$ and $I_A$, representing the interpretations for individual and
place-holder variables, respectively. 

$\text{BDD}_R \left( R, I_P \right)$ distinguishes three cases:
\begin{enumerate}
    \item{If $R$ is a relational variable, $I_P \left( R \right)$ is returned.}
    \item{
        If $R$ is a $\lambda$-abstraction, an OBDD with substituted variables is
        returned, i.e. $\langle x_1 \leftarrow d_1, \ldots, x_n \leftarrow d_n \rangle$.
    }
    \item{
        If $R$ is fixed point computation, a helper function is called. This
        helper function computes the fixed point by a series of approximations
        $T_1, \ldots, T_k$, starting from an empty relation and terminating as
        soon as $T_{i+1} = T_i$, which can be tested easily using OBDD because
        equivalence testing can be done in constant time.
    }
\end{enumerate}

The complete code of $\text{BDD}_f$ and $\text{BDD}_R$ is listed
in~\cite[p. 432]{BURCH90}. Based upon these two functions and the $\mu$-Calculus, one can now do CTL model
checking by representing a CTL formula $f$ as a $\mu$-Calculus relational term
$R$.

Assuming standard CTL syntax and semantics, i.e. for the set of atomic
propositions $AP$
\begin{enumerate}
    \item{$p \in AP$ is a CTL formula.}
    \item{
        Given two CTL formulas $f_1$ and $f_2$, also $\neg f_1$, $f_1 \land f_2$,
        $\text{\textbf{EX}} f_1$,
        $\text{\textbf{E}} \lbrack f_1 \text{\textbf{U}} f_2 \rbrack$ and
        $\text{\textbf{EG}} f_1$ are CTL formulas.
    }
\end{enumerate}

A CTL formula is represented by a \textit{Kripke structure} \newline
$\mathcal{M} \left( \mathcal{S}, \mathcal{S}_0, R, L \right)$, where $\mathcal{S}$
is a finite set of states over $AP$, $\mathcal{S}_0 \in \mathcal{S}$ is the set
of initial states, $R \subseteq \mathcal{S} \times \mathcal{S}$ is the transition
relation and $L: \mathcal{S} \rightarrow 2^{AP}$ is a labelling function. An
infinite sequence of states $\pi = s_0s_1s_2\ldots$ is called \textit{path} if
$\forall i \in \mathbb{N}.\left( s_i, s_{i+1} \right) \in R$.

Now, equivalent $\mu$-Calculus relational terms need to be formulated for
$\text{\textbf{EX}} f$,
$\text{\textbf{E}} \lbrack f_1 \text{\textbf{U}} f_2 \rbrack$ and
$\text{\textbf{EG}} f$. A CTL formula $f$ is \textbf{True} at state $s$ iff
$R \left( s \right)$ is \textbf{True}. 

$\text{\textbf{EX}} f$ is satisfied for a state $s$ iff a state satisfying $f$
exists and $R \left( s, t \right)$ holds. In terms of $\mu$-Calculus relational
terms, $\text{\textbf{EX}} f$ can be formulated as
$\lambda u \lbrack \exists v \left( f(v) \land R \left( u, v \right) \right) \rbrack$~\cite{BURCH90}.

The other two formulas are represented by fixed point characterizations, computed
using direct iteration or iterative squaring (see~\cite[p. 433]{BURCH90}).

$\text{\textbf{E}} \lbrack f_1 \text{\textbf{U}} f_2 \rbrack$ is satisfied for a
state $s$ if $f_2$ is satisfied by $s$ or $s$ satisfies $f_1$ and $s$ has a
successor state $t$ such that a path exists starting at $t$ and satisfying
$f_1 \text{\textbf{U}} f_2$. The $\mu$-Calculus representation (lfp) is defined by
$\mu Q \lbrack f_2 \lor \left( f_1 \land \text{\textbf{EX}} Q \right) \rbrack$~\cite{BURCH90}.

$\text{\textbf{EG}} f$ is satisfied for a state $s$ if $s$ satisfies $f$ and there
exists a successor state $t$ satisfying $f$ globally. Hence, the equivalent 
gfp representation is
$\nu Q \lbrack f \land \text{\textbf{EX}} Q \rbrack$~\cite{BURCH90}.

Utilizing these equivalent representations, one efficient CTL model checking.
Moreover, Burch, Clarke, McMillan and Dill~\cite{BURCH90-2} described how to use
these methods to do \textit{fair} CTL model checking. \tbr

\section{Alternative Representations}
\label{sec:alternative-representations}

In this last section, some alternative representations of BDDs are discussed
briefly, e.g. BMDs or BEDs. These representations were introduced primarily
because the OBDD representation is insufficient for some problems of practical
relevance. Bryant~\cite{BRYANT95} already reviewed most alternative represenations
but some were introduced after this overview, e.g. the
\textit{Binary Expression Diagrams} introduced by Andersen and
Hulgaard~\cite{ANDERSEN97}. \tbr

\paragraph*{Free BDDs}
\mbox{} % workaround to get line break after paragraph

Although the variable ordering problem is well-studied, OBDDs have some severe
limitations for some classes of Boolean functions, regardless of the variable
ordering chosen, e.g. integer multiplication (see Section~\ref{sec:limitations}).
One way researchers tried to overcome this issue is relaxing the variable ordering
requirement. \textit{Free BDDs} (\textit{FDDs}) is one proposed approach. FDDs
do not require the variables in the BDD to be ordered but, instead, require that
each variable only occurs once along any path (starting from the root node).
Hence, FDDS are also referred to as \textit{1-time branching programs}. Moreover,
FDDs have the same desirable algorithmic properties as OBDDs~\cite{BRYANT95}. \tbr

\paragraph*{Zero-Suppressed BDDs}
\mbox{} % workaround to get line break after paragraph

This representation is especially useful when the data can be represented as
(sparse) bit vectors. Any subset of these bit vectors can be represented by a
Boolean function over $n$ variables. If the corresponding bit vector is in the
set, the \textit{Zero-Suppressed BDDs} (\textit{ZBDDs}) yields 1 for a given
variable assignment.

The bit vector are \textit{sparse} in the sense that the set contains much less
than $2^n$ vectors and the bit vectors themselves have many zero elements. ZBDDs
are similar to OBDDs but they use a different reduction rule, exploiting the
sparse bit vectors. In ZBDDs, nodes can be eliminated whenever setting the
corresponding variable to 1 yields 0 as result, which is often the case for sparse
bit vectors. This representation proved successful to solve some combinatorial
problems~\cite{BRYANT95}. \tbr %\newpage

\paragraph*{Binary Expression Diagrams}
\mbox{} % workaround to get line break after paragraph

In 1997, Andersen and Hulgaard~\cite{ANDERSEN97} proposeded extending OBDDs with
a third type of nodes representing \textit{operators}. The resulting
representation is referred to as \textit{Binary Expression Diagram}
(\textit{BED}). Such an operator node $n$ has one attribute $op(n)$, the binary
Boolean operator, and, like nonterminal nodes, two children $lo(n)$ and $hi(n)$.
Operator nodes represent their corresponding Boolean connectives in the BED
graph~\cite{ANDERSEN97}.

Just like OBDDs, BEDs share equivalent Boolean subexpressions. Furthermore, BEDs
have their own set of manipulating operations, especially because of the operator
nodes. For example, the \textit{up-step} operation moves variables up in the BED
hierarchy, if possible. As an effect, operator nodes are pushed towards the
terminal nodes and some operator nodes may be evaluated if some of the associated
expressions are terminal nodes. The BED may be simplified afterwards. These steps
can be repeated to reduce the BED over and over again, and simplifying some tests,
e.g. tautology testing. It is also possible to move all variable up in a single
step. Nevertheless, reducing the BED step-by-step can exploit structural
information~\cite{ANDERSEN97}.

In contrast to OBDDs, BEDs are \textit{exponentially more succinct} because
arbitrary circuits can be translated into BEDs. As a consequence, the BED
representation suffices to solve integer multiplication using a quadratic number
of nodes~\cite{ANDERSEN97}. 

Analogous to OBDDs and FDDs, OBEDs and FBEDs can be defined, whereas OBEDs are
OBDDs without operator nodes. The reduction rule of BEDs is similar to the one
of OBDDs but extended with an additional rule for operator nodes. For any operator
nodes $n$ it has to hold that $lo(n)$ and $hi(n)$ are nonterminals in order to
guarantee reducedness. Apart from that, before generating a new node, an existence
check is performed. If there exists an equivalent node, this node is reused.
Otherwise the new node is created and considered in the next check. However, even
if a BED is reduced, canonicity is not guaranteed (in contrast to
OBDDs)~\cite{ANDERSEN97}.

Andersen and Hulgaard~\cite{ANDERSEN97} also introduced some checks especially
for operator nodes in order to reduce the BED-size even further. Moreover a
detailed description of how to convert a BED into an equivalent OBDD is provided,
distinguishing between step-by-step variable moving (denoted as \textit{up\_one})
and block-wise variable moving (denoted as \textit{up\_all}).
 
The \textit{up\_one} operation can be implemented with expected runtime of $O(n)$,
where $n$ denotes the number reachable of nodes from a given node. Nonetheless,
because BEDs are exponentially more succinct than OBDDs, the complete
transformation has an exponential worst-case complexity. Also \textit{up\_all} has
an exponential worst-case complexity (for the same reason)~\cite{ANDERSEN97}.

Andersen and Hulgaard~\cite{ANDERSEN97} concluded that \textit{up\_all} is a
memory efficient approach to construct OBDDs in contrast to the OBDD construction
from scratch, without actually increasing the runtime. \textit{up\_one} is used
to implement existential quantification and substitution efficiently, having
linear runtime complexity in the size of the BED. Reconsidering symbolic model
checking, BEDs can be used to evaluate fixed point computations directly without
converting it to an OBDD. Moreover, BEDs can perform fixed point computations more
efficiently because of their succinct representation~\cite{ANDERSEN97}. \tbr

%\clearpage
% balance columns on last page
%\balance

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}